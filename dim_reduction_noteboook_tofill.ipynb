{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Workshop\n",
    "\n",
    "## Example : hurricane meteorological data\n",
    "\n",
    "Sophie Giffard-Roisin (sophie.giffard@colorado.edu)\n",
    "\n",
    "## Introduction\n",
    "This is an initiation to introduce dimensionality reduction and get you to know how it works.\n",
    "\n",
    "We will use a real hurricane meteorological dataset, which typical goal is to estimate the current stength of the hurricane or to predict its evolution. \n",
    "<img src=\"https://github.com/sophiegif/ramp_kit_storm_forecast_new/blob/master/figures_pynb/all_storms_since1979_IBTrRACKS_newcats.png?raw=true\" width=\"70%\">\n",
    "<div style=\"text-align: center\">Database: tropical/extra-tropical storm tracks since 1979. Dots = initial position, color = maximal storm strength according to the Saffir-Simpson scale.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "* numpy  \n",
    "* matplotlib\n",
    "* pandas \n",
    "* scikit-learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_filename = 'data/train.csv'\n",
    "data = pd.read_csv(train_filename)\n",
    "y_df = data['windspeed']\n",
    "X_df = data.drop(['windspeed', 'stormid', 'instant_t'], axis=1)\n",
    "X_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the meaning of the columns, refer to this notebook https://github.com/ramp-kits/storm_forecast/blob/master/storm_forecast_starting_kit.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load also the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'data/test.csv'\n",
    "data_test = pd.read_csv(train_filename)\n",
    "y_df_test = data_test['windspeed']\n",
    "X_df_test = data_test.drop(['windspeed', 'stormid', 'instant_t'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_df = X_df.astype(float)\n",
    "X_df_test = X_df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all features, transform your data such that mean=0 and std=1 (on the training data), and use the same parameters for transforming the test data also. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_df =\n",
    "X_df_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component analysis\n",
    "\n",
    "Use PCA and transform the training data such as to conserve 95% of the explained variance of the data. Then, transform the test data accordingly. (Look at sklearn.decomposition.PCA for how to use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fit the pca with the training features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculate the cumulative explained variance (you can use the np.cumsum function) and determine how many modes are necessary in order to keep 95% of the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum_var = \n",
    "\n",
    "thresh =\n",
    "\n",
    "nummodes ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(cumsum_var)\n",
    "plt.axhline(thresh, xmin=0, xmax=len(pca.explained_variance_))\n",
    "plt.show()\n",
    "print('Number of modes:' + str(num_modes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create a reduced feature matrix X_df_pca (and then X_df_test_pca) using the number of modes found. You may need to create a second 'pca' instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_df_pca = \n",
    "X_df_test_pca = \n",
    "\n",
    "print('Number of feature dimensions:'+ str(len(X_df_pca[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the first two modes of the X_df_pca, with y as color label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.scatter(np.transpose(X_df_pca)[0], np.transpose(X_df_pca)[1], c=y_df, s=1, cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.scatter(np.transpose(X_df_pca)[0], np.transpose(X_df_pca)[1], c=y_df, s=1, cmap='jet')\n",
    "ax2.set_title('Without outliers')\n",
    "plt.xlim([-4,4])\n",
    "col = plt.colorbar()\n",
    "t = plt.suptitle('PCA modes 0 and 1, color = y (hurricane windspeed, knots):')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear methods: Multidimensional scaling (MDS) and Isomap\n",
    "\n",
    "The MDS performs a non-linear dimentionality reduction  by preserving the (Eucliean) distances between points. The isomap is an extended version of the MDS where the geodesic distances are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS, Isomap\n",
    "Nsamples = 1000\n",
    "X_df_small = X_df[:Nsamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, apply the MDS to the training data X_df with 2 components and save it as X_df_MDS. Do the same thing with isomap and create a X_df_isomap. Verify that their shape are Nb_samples x 2 . Use less samples (1000) in order to reduce computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_df_MDS = \n",
    "\n",
    "X_df_isomap = \n",
    "\n",
    "print(X_df_MDS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.set_title('MDS with 2 components')\n",
    "plt.scatter(np.transpose(X_df_MDS)[0], np.transpose(X_df_MDS)[1], c=y_df[:Nsamples], s=1, cmap='jet')\n",
    "c=plt.colorbar()\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "ax2.set_title('Isomap with 2 components')\n",
    "plt.scatter(np.transpose(X_df_isomap)[0], np.transpose(X_df_isomap)[1], c=y_df[:Nsamples], s=1, cmap='jet')\n",
    "c2=plt.colorbar()\n",
    "\n",
    "\n",
    "t = plt.suptitle('MDS and Isomap, color = y (hurricane windspeed, knots):')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably see on the Isomap the trajectories of individual hurricanes (one hurricane has between 5 to 100 time steps, every time step is a different point here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARD : Regression with automatic dim. reduction\n",
    "\n",
    "Choose a smaller number of samples in order to reduce the computational time... to be noticed, there are faster versions available here : https://github.com/AmazaspShumik/sklearn-bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression\n",
    "Nsamples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit an ARD instance with part of the training samples (ex. 500 - you can shuffle them to have a better result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the values of the feature weights and their histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.title(\"Weights of the model\")\n",
    "ax = plt.subplot(1,2,1)\n",
    "plt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,\n",
    "         label=\"ARD estimate\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Values of the weights\")\n",
    "plt.legend(loc=1)\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "plt.title(\"Histogram of the weights\")\n",
    "plt.hist(clf.coef_, bins=len(X_df[0]), color='navy', log=True)\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Values of the weights\")\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see on the first figure what are the important features for this task. On the second, you can see that a lot of weights are 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, estimate the mean absolute error on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "You can play with other dimensionality reduction on the same dataset (or on others) by looking at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim # for deciding whether to use it or not\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.manifold import SpectralEmbedding"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
